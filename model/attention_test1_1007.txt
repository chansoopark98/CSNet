import efficientnet.keras as efn
import tensorflow as tf
from tensorflow import keras
from keras.layers import Conv2D, BatchNormalization, Activation, Dense, Concatenate, Flatten, Reshape, Dropout, Add
from keras.regularizers import l2

source_layers_to_extract = {
    'B0': ['block3b_add', 'block5c_add', 'block7a_project_bn'],
    'B1': ['block3c_add', 'block5d_add', 'block7b_add'],
    'B2': ['block3c_add', 'block5d_add', 'block7b_add'],
    'B3': ['block3c_add', 'block5e_add', 'block7b_add'],
    'B4': ['block3d_add', 'block5f_add', 'block7b_add'],
    'B5': ['block3e_add', 'block5g_add', 'block7c_add'],
    'B6': ['block3f_add', 'block5h_add', 'block7c_add'],
    'B7': ['block3g_add', 'block5j_add', 'block7d_add'],
}

#    f   k  s    p
extra_layers_params = [[(128, 1, 1, 'same'), (256, 3, 2, 'same')],
                       [(128, 1, 1, 'same'), (256, 3, 1, 'valid')],
                       [(128, 1, 1, 'same'), (256, 3, 1, 'valid')]]


def remove_dropout(model):
    for layer in model.layers:
        if isinstance(layer, Dropout):
            layer.rate = 0
    model_copy = keras.models.clone_model(model)
    model_copy.set_weights(model.get_weights())
    del model
    return model_copy


def add_extras(extras, x, regularization=5e-4):
    # 5,5 / 3,3 / 1,1 세 개 수행
    features = []
    for extra_layer in extras:
        x = add_layer(extra_layer[0], x, regularization)
        x = add_layer(extra_layer[1], x, regularization)
        features.append(x)
    return features


def add_layer(layer_params, x, regularization=5e-4):
    filters, kernel_size, stride, padding = layer_params
    x = Conv2D(filters, kernel_size, stride, padding=padding, kernel_regularizer=l2(regularization))(x)
    x = Activation('relu')(x)
    return x


def create_base_model(base_model_name, pretrained=True, IMAGE_SIZE=[300, 300]):
    if pretrained is False:
        weights = None
    else:
        weights = "imagenet"
    if base_model_name == 'B0':
        base = efn.EfficientNetB0(weights=weights, include_top=False, input_shape=[*IMAGE_SIZE, 3])
    elif base_model_name == 'B1':
        base = efn.EfficientNetB1(weights=weights, include_top=False, input_shape=[*IMAGE_SIZE, 3])
    elif base_model_name == 'B2':
        base = efn.EfficientNetB2(weights=weights, include_top=False, input_shape=[*IMAGE_SIZE, 3])
    elif base_model_name == 'B3':
        base = efn.EfficientNetB3(weights=weights, include_top=False, input_shape=[*IMAGE_SIZE, 3])
    elif base_model_name == 'B4':
        base = efn.EfficientNetB4(weights=weights, include_top=False, input_shape=[*IMAGE_SIZE, 3])
    elif base_model_name == 'B5':
        base = efn.EfficientNetB5(weights=weights, include_top=False, input_shape=[*IMAGE_SIZE, 3])
    elif base_model_name == 'B6':
        base = efn.EfficientNetB6(weights=weights, include_top=False, input_shape=[*IMAGE_SIZE, 3])
    elif base_model_name == 'B7':
        base = efn.EfficientNetB7(weights=weights, include_top=False, input_shape=[*IMAGE_SIZE, 3])
    base = remove_dropout(base)
    base.trainable = True
    return base


def create_backbone(base_model_name, pretrained=True, IMAGE_SIZE=[300, 300], regularization=5e-4):
    source_layers = []
    base = create_base_model(base_model_name, pretrained, IMAGE_SIZE)
    print(base)
    layer_names = source_layers_to_extract[base_model_name]
    print("layer_names : ", layer_names)

    # get extra layer
    conv75 = base.get_layer('block2b_add').output  # 75 75 24
    conv38 = base.get_layer(layer_names[0]).output
    conv19 = base.get_layer(layer_names[1]).output
    conv10 = base.get_layer(layer_names[2]).output

    conv75_down = Conv2D(24, (1, 1), (1, 1), padding='SAME', kernel_regularizer=l2(0.0005),
                         kernel_initializer='he_normal', name='extra_conv75_1')(conv75)
    conv75_down = Activation('relu')(conv75_down)

    conv75_down = Conv2D(40, (3, 3), (2, 2), padding='SAME', kernel_regularizer=l2(0.0005),
                         kernel_initializer='he_normal', name='extra_conv75_2')(conv75_down)
    conv75_down = BatchNormalization(axis=3, name="conv75_dense_batchNorm")(conv75_down)
    conv75_down = Activation('relu')(conv75_down)

    conv75_down = Conv2D(40, (3, 3), (1, 1), padding='SAME', kernel_regularizer=l2(0.0005),
                         kernel_initializer='he_normal', name='extra_conv75_3')(conv75_down)
    conv75_down = Activation('relu')(conv75_down)

    # tf.math.reduce_mean(conv75, ax)

    conv75_channel = conv75_down.get_shape()[-1]
    print('conv75_channel', conv75_channel)

    conv75_down_avg_channel = tf.reduce_mean(conv75_down, axis=[1, 2], keepdims=True)
    conv75_down_max_channel = tf.reduce_max(conv75_down, axis=[1, 2], keepdims=True)

    assert conv75_down_avg_channel.get_shape()[1:] == (1, 1, conv75_channel)
    conv75_avg_dense = Dense(units=conv75_channel,
                             activation=tf.nn.relu,
                             kernel_initializer=tf.keras.initializers.VarianceScaling(),
                             bias_initializer=tf.keras.initializers.constant(value=0.0),
                             name='channel_attention_0')(conv75_down_avg_channel)

    conv75_avg_dense = Dense(units=conv75_channel,
                             activation=tf.nn.relu,
                             kernel_initializer=tf.keras.initializers.VarianceScaling(),
                             bias_initializer=tf.keras.initializers.constant(value=0.0),
                             name='channel_attention_1')(conv75_avg_dense)

    assert conv75_down_max_channel.get_shape()[1:] == (1, 1, conv75_channel)
    conv75_max_dense = Dense(units=conv75_channel,
                             activation=tf.nn.relu,
                             name='channel_attention_2')(conv75_down_max_channel)

    assert conv75_max_dense.get_shape()[1:] == (1, 1, conv75_channel)
    conv75_max_dense = Dense(units=conv75_channel,
                             activation=tf.nn.relu,
                             name='channel_attention_3')(conv75_max_dense)

    print('conv75_avg_dense', conv75_avg_dense)
    print('conv75_max_dense', conv75_max_dense)

    conv75_sigmoid = tf.sigmoid(conv75_avg_dense + conv75_max_dense, 'conv75_attention_sigmoid')

    print('conv75_sigmoid', conv75_sigmoid)

    conv75_ca = tf.math.multiply(conv75_down, conv75_sigmoid, name='conv75_multiply')

    conv75_down_avg_spatial = tf.reduce_mean(conv75_ca, axis=[3], keepdims=True)
    assert conv75_down_avg_spatial.get_shape()[-1] == 1

    conv75_down_max_spatial = tf.reduce_max(conv75_ca, axis=[3], keepdims=True)
    assert conv75_down_max_spatial.get_shape()[-1] == 1

    concat = tf.concat([conv75_down_avg_spatial, conv75_down_max_spatial], 3)
    assert concat.get_shape()[-1] == 2

    concat = Conv2D(1, (7, 7), (1, 1), padding='same', activation=None,
                    kernel_initializer=tf.keras.initializers.variance_scaling(),
                    use_bias=False, name='spatial_conv')(concat)
    assert concat.get_shape()[-1] == 1

    concat = tf.sigmoid(concat, 'sigmoid_spatial')
    conv75_sa = tf.math.multiply(conv75_ca, concat, name='conv75_spatial')

    # conv75_down = Conv2D(24, (1, 1), (1, 1), padding='SAME', kernel_regularizer=l2(0.0005),
    #                      kernel_initializer='he_normal', name='extra_conv75_1')(conv75)
    # conv75_down = Activation('relu')(conv75_down)
    # conv75_down = Conv2D(40, (3, 3), (2, 2), padding='SAME', kernel_regularizer=l2(0.0005),
    #                      kernel_initializer='he_normal', name='extra_conv75_2')(conv75_down)
    # conv75_down = Activation('relu')(conv75_down)
    # conv75_down = Conv2D(40, (3, 3), (1, 1), padding='SAME', kernel_regularizer=l2(0.0005),
    #                      kernel_initializer='he_normal', name='extra_conv75_3')(conv75_down)
    # conv75_down = Activation('relu')(conv75_down)

    # INPUT = 38 X 38 X 40  source_layers_0, # block3b_add/add_1:0    38, 38, 40
    # 38 to 19
    conv38_down = Conv2D(112, (3, 3), (2, 2), padding='SAME', kernel_regularizer=l2(0.0005),
                         kernel_initializer='he_normal', name='extra_conv38_1')(conv38)
    conv38_down = BatchNormalization(axis=3, name="conv38_down_bn")(conv38_down)
    conv38_down = Activation('relu')(conv38_down)

    conv38_down = Conv2D(112, (3, 3), (1, 1), padding='SAME', kernel_regularizer=l2(0.0005),
                         kernel_initializer='he_normal', name='extra_conv38_2')(conv38_down)
    conv38_down = Activation('relu')(conv38_down)

    # Feature fusion conv38 + conv19
    conv38 = Add()([conv75_sa, conv38])
    conv19 = Add()([conv38_down, conv19])

    print("test shape")

    print('conv75_sa', conv75_sa)
    print('conv38', conv38)
    print('conv38_down', conv38_down)
    print('conv19', conv19)

    source_layers.append(conv38)
    source_layers.append(conv19)
    source_layers.append(conv10)
    # original code
    # for name in layer_names:
    #     source_layers.append(base.get_layer(name).output)
    
    x = source_layers[-1]

    # source_layers_0, # block3b_add/add_1:0    38, 38, 40
    # source_layers_1, # block5c_add/add_1:0    19, 19, 112
    # source_layers_2, # block7a_project_bn/cond_1/Identity:0    10, 10, 320

    source_layers.extend(add_extras(extra_layers_params, x))
    return base.input, source_layers

